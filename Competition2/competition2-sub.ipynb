{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNnqREAD29B-",
        "outputId": "8544eaf5-e660-4884-83c5-44c69fe5319a"
      },
      "outputs": [],
      "source": [
        "# 普通に使うだけの場合は、colabの環境をGPUに変更後、id_dicにtest, train, sample_submissionのcsvのidを入れて実行してください\n",
        "# 今回の実行は普通に行うととても時間がかかるので注意してください。\n",
        "# 実行完了後、save_items.zipがダウンロードされます。\n",
        "# この中にsubmit.csvとfoldで作られたモデル(再現性確保のため)が入ってます。\n",
        "\n",
        "\n",
        "\n",
        "######################\n",
        "# 再現性を確保するときのみ以下の手順を行ってください。\n",
        "######################\n",
        "\n",
        "#if you use save, uncomment and enter id ##\n",
        "# 今回GPUを使っているためか、実行ごとで精度がわずかながら変わります。\n",
        "# そのため完璧な再現性を得るためには、use_save_modelのコメントアウトをもどして、model_idにモデルを保存したもののzipファイル(save_items.zip)のidいれてください。\n",
        "\n",
        "# ここのコメントアウトを解除\n",
        "# use_save_model = True\n",
        "model_id = ''\n",
        "######################\n",
        "######################\n",
        "\n",
        "\n",
        "#######for colab##########\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# ここにvalueを入れる。\n",
        "id_dic={\n",
        "    'test.csv':'', \n",
        "    'train.csv':'', \n",
        "    'sample_submission.csv':'',\n",
        "    }\n",
        "\n",
        "\n",
        "#######\n",
        "dir_name = 'use_model_dir.zip'\n",
        "try:\n",
        "    use_save_model\n",
        "    if use_save_model:\n",
        "        id_dic[dir_name] = model_id\n",
        "    else:\n",
        "        use_save_model = False\n",
        "except NameError:\n",
        "    use_save_model = False\n",
        "#######\n",
        "for k,v in id_dic.items():\n",
        "    downloaded = drive.CreateFile({'id': v})\n",
        "    downloaded.GetContentFile(f'{k}')\n",
        "#######\n",
        "if use_save_model:\n",
        "    !mkdir use_model_dir\n",
        "    !unzip use_model_dir.zip -d use_model_dir\n",
        "    !rm use_model_dir.zip\n",
        "else:\n",
        "    pass\n",
        "\n",
        "#########to save items#############\n",
        "!mkdir save_items\n",
        "#########to use lightgbm gpu #########\n",
        "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "%cd /content/LightGBM/\n",
        "!mkdir build\n",
        "!cmake -DUSE_GPU=1 #avoid ..\n",
        "!make -j$(nproc)\n",
        "!sudo apt-get -y install python-pip\n",
        "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
        "%cd /content/LightGBM/python-package\n",
        "!sudo python setup.py install --precompile\n",
        "##############################\n",
        "%cd /content\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MHZOYou3F-0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from contextlib import contextmanager\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "from scipy.stats import kurtosis, iqr, skew\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def main(debug= False):\n",
        "    num_rows = 30000 if debug else None\n",
        "    with timer(\"application_train and application_test\"):\n",
        "        df = get_train_test(DATA_DIRECTORY, num_rows= num_rows)\n",
        "        print(\"Application dataframe shape: \", df.shape)\n",
        "\n",
        "    df = reduce_memory(df)   \n",
        "    lgbm_categorical_feat = [\n",
        "    'CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE',\n",
        "    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE',\n",
        "    'ORGANIZATION_TYPE', 'NAME_TYPE_SUITE']\n",
        "    with timer(\"Run LightGBM\"):\n",
        "        feat_importance = kfold_lightgbm_sklearn(df, lgbm_categorical_feat)\n",
        "        print(feat_importance)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------- LIGHTGBM MODEL -------------------------\n",
        "\n",
        "def kfold_lightgbm_sklearn(data, categorical_feature = None):\n",
        "    df = data[data['TARGET'].notnull()]\n",
        "    test = data[data['TARGET'].isnull()]\n",
        "    print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n",
        "    del_features = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index', 'level_0']\n",
        "    predictors = list(filter(lambda v: v not in del_features, df.columns))\n",
        "\n",
        "    if not STRATIFIED_KFOLD:\n",
        "        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
        "    else:\n",
        "        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
        "\n",
        "    # Hold oof predictions, test predictions, feature importance and training/valid auc\n",
        "    oof_preds = np.zeros(df.shape[0])\n",
        "    sub_preds = np.zeros(test.shape[0])\n",
        "    importance_df = pd.DataFrame()\n",
        "    eval_results = dict()\n",
        "\n",
        "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['TARGET'])):\n",
        "        train_x, train_y = df[predictors].iloc[train_idx], df['TARGET'].iloc[train_idx]\n",
        "        valid_x, valid_y = df[predictors].iloc[valid_idx], df['TARGET'].iloc[valid_idx]\n",
        "\n",
        "\n",
        "        if use_save_model:\n",
        "            clf = pickle.load(open(f'{PRE_MODEL_DIR}/FOLD{n_fold}_.pickle', 'rb'))\n",
        "        else:\n",
        "            params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n",
        "            clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n",
        "            if not categorical_feature:\n",
        "                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
        "                        eval_metric='auc', verbose=400, early_stopping_rounds= EARLY_STOPPING)\n",
        "            else:\n",
        "                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
        "                        eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING,\n",
        "                        feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n",
        "            pickle.dump(clf, open(f'save_items/FOLD{n_fold}_.pickle', 'wb'))\n",
        "            clf = pickle.load(open(f'save_items/FOLD{n_fold}_.pickle', 'rb'))\n",
        "\n",
        "\n",
        "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
        "        sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
        "\n",
        "        # Feature importance by GAIN and SPLIT\n",
        "        fold_importance = pd.DataFrame()\n",
        "        fold_importance[\"feature\"] = predictors\n",
        "        fold_importance[\"gain\"] = clf.booster_.feature_importance(importance_type='gain')\n",
        "        fold_importance[\"split\"] = clf.booster_.feature_importance(importance_type='split')\n",
        "        importance_df = pd.concat([importance_df, fold_importance], axis=0)\n",
        "        eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n",
        "        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n",
        "\n",
        "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
        "        del clf, train_x, train_y, valid_x, valid_y\n",
        "        gc.collect()\n",
        "\n",
        "    print('Full AUC score %.6f' % roc_auc_score(df['TARGET'], oof_preds))\n",
        "    test['TARGET'] = sub_preds.copy()\n",
        "\n",
        "    # Get the average feature importance between folds\n",
        "    mean_importance = importance_df.groupby('feature').mean().reset_index()\n",
        "    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n",
        "    # Save feature importance, test predictions and oof predictions as csv\n",
        "    if GENERATE_SUBMISSION_FILES:\n",
        "\n",
        "        # Generate oof csv\n",
        "        oof = pd.DataFrame()\n",
        "        oof['SK_ID_CURR'] = df['SK_ID_CURR'].copy()\n",
        "        df['PREDICTIONS'] = oof_preds.copy()\n",
        "        df['TARGET'] = df['TARGET'].copy()\n",
        "        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
        "        # Save submission (test data) and feature importance\n",
        "\n",
        "        # change\n",
        "        test[['SK_ID_CURR', 'TARGET']].to_csv('save_items/submission.csv', index=False)\n",
        "\n",
        "        # test[['SK_ID_CURR', 'TARGET']].to_csv('submission{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
        "        mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
        "    return mean_importance\n",
        "\n",
        "\n",
        "# ------------------------- APPLICATION PIPELINE -------------------------\n",
        "\n",
        "def get_train_test(path, num_rows = None):\n",
        "    \"\"\" Process application_train.csv and application_test.csv and return a pandas dataframe. \"\"\"\n",
        "    train = pd.read_csv(os.path.join(path, 'train.csv'), nrows= num_rows)\n",
        "    test = pd.read_csv(os.path.join(path, 'test.csv'), nrows= num_rows)\n",
        "    df = train.append(test)\n",
        "    del train, test; gc.collect()\n",
        "    # Data cleaning\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']  # 4 people with XNA code gender\n",
        "\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
        "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
        "\n",
        "    # Categorical age - based on target=1 plot\n",
        "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
        "\n",
        "    # New features based on External sources\n",
        "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
        "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
        "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
        "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
        "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
        "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
        "\n",
        "    # Credit ratios\n",
        "    df['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
        "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
        "    # Income ratios\n",
        "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
        "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
        "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
        "    # Time ratios\n",
        "    df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
        "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
        "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
        "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
        "\n",
        "    # Groupby: Statistics for applications in the same group\n",
        "    group = ['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n",
        "    df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n",
        "    df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n",
        "    df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n",
        "    df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n",
        "    df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n",
        "    df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n",
        "    df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n",
        "    df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n",
        "    df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')\n",
        "\n",
        "    # Encode categorical features (LabelEncoder)\n",
        "    df, le_encoded_cols = label_encoder(df, None)\n",
        "    df = drop_application_columns(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "def drop_application_columns(df):\n",
        "    \"\"\" Drop features based on permutation feature importance. \"\"\"\n",
        "\n",
        "\n",
        "    drop_list = [\n",
        "        'CNT_CHILDREN', 'CNT_FAM_MEMBERS',\n",
        "        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n",
        "        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
        "        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
        "        'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
        "    ]\n",
        "\n",
        "    df.drop(drop_list, axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_age_label(days_birth):\n",
        "    \"\"\" Return the age group label (int). \"\"\"\n",
        "    age_years = -days_birth / 365\n",
        "    if age_years < 27: return 1\n",
        "    elif age_years < 40: return 2\n",
        "    elif age_years < 50: return 3\n",
        "    elif age_years < 65: return 4\n",
        "    elif age_years < 99: return 5\n",
        "    else: return 0\n",
        "\n",
        "# ------------------------- UTILITY FUNCTIONS -------------------------\n",
        "\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n",
        "\n",
        "\n",
        "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
        "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
        "                               for e in agg_df.columns.tolist()])\n",
        "    return agg_df.reset_index()\n",
        "\n",
        "\n",
        "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
        "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
        "\n",
        "\n",
        "def do_mean(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def do_median(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def do_std(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def do_sum(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n",
        "    \"\"\"Create a new column for each categorical value in categorical columns. \"\"\"\n",
        "    original_columns = list(df.columns)\n",
        "    if not categorical_columns:\n",
        "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
        "    categorical_columns = [c for c in df.columns if c not in original_columns]\n",
        "    return df, categorical_columns\n",
        "\n",
        "\n",
        "def label_encoder(df, categorical_columns=None):\n",
        "    \"\"\"Encode categorical values as integers (0,1,2,3...) with pandas.factorize. \"\"\"\n",
        "    if not categorical_columns:\n",
        "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    for col in categorical_columns:\n",
        "        df[col], uniques = pd.factorize(df[col])\n",
        "    return df, categorical_columns\n",
        "\n",
        "\n",
        "\n",
        "def reduce_memory(df):\n",
        "    \"\"\"Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
        "          .format(start_mem, len(df.columns)))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type != object:\n",
        "            cmin = df[col].min()\n",
        "            cmax = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                # Can use unsigned int here too\n",
        "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
        "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
        "    return df\n",
        "\n",
        "# ------------------------- CONFIGURATIONS -------------------------\n",
        "\n",
        "# GENERAL CONFIGURATIONS\n",
        "NUM_THREADS = 4\n",
        "DATA_DIRECTORY = \"./\"\n",
        "PRE_MODEL_DIR = '/content/use_model_dir/content/save_items'\n",
        "SUBMISSION_SUFIX = \"for_GCI\"\n",
        "\n",
        "# INSTALLMENTS TREND PERIODS\n",
        "INSTALLMENTS_LAST_K_TREND_PERIODS =  [12, 24, 60, 120]\n",
        "\n",
        "# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\n",
        "GENERATE_SUBMISSION_FILES = True\n",
        "STRATIFIED_KFOLD = False\n",
        "\n",
        "RANDOM_SEED = 737851\n",
        "NUM_FOLDS = 10\n",
        "EARLY_STOPPING = 2500\n",
        "\n",
        "\n",
        "\n",
        "LIGHTGBM_PARAMS = { 'n_estimators': 100000, 'device' : 'gpu', 'subsample':1,'verbosity': -1, 'learning_rate': 0.001134, 'boosting': 'goss', 'num_leaves': 51, 'reg_alpha': 0.9957684513557233, 'reg_lambda': 0.9960067587898466, 'colsample_bytree': 0.17072069024645947, 'min_split_gain': 1.6977135811936967e-05, 'top_rate': 0.42562202305456304, 'other_rate': 0.543885726442511} \n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pd.set_option('display.max_rows', 60)\n",
        "    pd.set_option('display.max_columns', 100)\n",
        "    with timer(\"Pipeline total time\"):\n",
        "        main(debug= False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQgQHUCzCxHy"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/save_items.zip /content/save_items\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/save_items.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "forsub_compe2_SS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
