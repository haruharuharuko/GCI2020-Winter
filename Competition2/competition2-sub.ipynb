{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "forsub_compe2_SS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNnqREAD29B-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8544eaf5-e660-4884-83c5-44c69fe5319a"
      },
      "source": [
        "# 普通に使うだけの場合は、colabの環境をGPUに変更後、id_dicにtest, train, sample_submissionのcsvのidを入れて実行してください\n",
        "# 今回の実行は普通に行うととても時間がかかるので注意してください。\n",
        "# 実行完了後、save_items.zipがダウンロードされます。\n",
        "# この中にsubmit.csvとfoldで作られたモデル(再現性確保のため)が入ってます。\n",
        "\n",
        "\n",
        "\n",
        "######################\n",
        "# 再現性を確保するときのみ以下の手順を行ってください。\n",
        "######################\n",
        "\n",
        "#if you use save, uncomment and enter id ##\n",
        "# 今回GPUを使っているためか、実行ごとで精度がわずかながら変わります。\n",
        "# そのため完璧な再現性を得るためには、use_save_modelのコメントアウトをもどして、model_idにモデルを保存したもののzipファイル(save_items.zip)のidいれてください。\n",
        "\n",
        "# ここのコメントアウトを解除\n",
        "# use_save_model = True\n",
        "model_id = ''\n",
        "######################\n",
        "######################\n",
        "\n",
        "\n",
        "#######for colab##########\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# ここにvalueを入れる。\n",
        "id_dic={\n",
        "    'test.csv':'', \n",
        "    'train.csv':'', \n",
        "    'sample_submission.csv':'',\n",
        "    }\n",
        "\n",
        "\n",
        "#######\n",
        "dir_name = 'use_model_dir.zip'\n",
        "try:\n",
        "    use_save_model\n",
        "    if use_save_model:\n",
        "        id_dic[dir_name] = model_id\n",
        "    else:\n",
        "        use_save_model = False\n",
        "except NameError:\n",
        "    use_save_model = False\n",
        "#######\n",
        "for k,v in id_dic.items():\n",
        "    downloaded = drive.CreateFile({'id': v})\n",
        "    downloaded.GetContentFile(f'{k}')\n",
        "#######\n",
        "if use_save_model:\n",
        "    !mkdir use_model_dir\n",
        "    !unzip use_model_dir.zip -d use_model_dir\n",
        "    !rm use_model_dir.zip\n",
        "else:\n",
        "    pass\n",
        "\n",
        "#########to save items#############\n",
        "!mkdir save_items\n",
        "#########to use lightgbm gpu #########\n",
        "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "%cd /content/LightGBM/\n",
        "!mkdir build\n",
        "!cmake -DUSE_GPU=1 #avoid ..\n",
        "!make -j$(nproc)\n",
        "!sudo apt-get -y install python-pip\n",
        "!sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
        "%cd /content/LightGBM/python-package\n",
        "!sudo python setup.py install --precompile\n",
        "##############################\n",
        "%cd /content\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LightGBM'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 20791 (delta 2), reused 2 (delta 1), pack-reused 20781\u001b[K\n",
            "Receiving objects: 100% (20791/20791), 16.16 MiB | 13.30 MiB/s, done.\n",
            "Resolving deltas: 100% (15182/15182), done.\n",
            "Submodule 'include/boost/compute' (https://github.com/boostorg/compute) registered for path 'compute'\n",
            "Cloning into '/content/LightGBM/compute'...\n",
            "remote: Enumerating objects: 21728, done.        \n",
            "remote: Total 21728 (delta 0), reused 0 (delta 0), pack-reused 21728\n",
            "Receiving objects: 100% (21728/21728), 8.51 MiB | 8.80 MiB/s, done.\n",
            "Resolving deltas: 100% (17565/17565), done.\n",
            "Submodule path 'compute': checked out '36c89134d4013b2e5e45bc55656a18bd6141995a'\n",
            "/content/LightGBM\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Looking for CL_VERSION_2_2\n",
            "-- Looking for CL_VERSION_2_2 - found\n",
            "-- Found OpenCL: /usr/lib/x86_64-linux-gnu/libOpenCL.so (found version \"2.2\") \n",
            "-- OpenCL include directory: /usr/include\n",
            "-- Boost version: 1.65.1\n",
            "-- Found the following Boost libraries:\n",
            "--   filesystem\n",
            "--   system\n",
            "-- Performing Test MM_PREFETCH\n",
            "-- Performing Test MM_PREFETCH - Success\n",
            "-- Using _mm_prefetch\n",
            "-- Performing Test MM_MALLOC\n",
            "-- Performing Test MM_MALLOC - Success\n",
            "-- Using _mm_malloc\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/LightGBM\n",
            "\u001b[35m\u001b[1mScanning dependencies of target _lightgbm\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target lightgbm\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/main.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/application/application.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/boosting.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_model_text.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/train_share_states.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/gbdt_prediction.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/boosting/prediction_early_stop.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/bin.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/ifaddrs_patch.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/cuda_tree_learner.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/config_auto.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/_lightgbm.dir/src/c_api.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/dataset_loader.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/file_io.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/json11.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/metadata.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/parser.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/train_share_states.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/io/tree.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/dcg_calculator.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/metric/metric.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/ifaddrs_patch.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linker_topo.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_mpi.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/linkers_socket.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/network/network.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/objective/objective_function.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/cuda_tree_learner.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/data_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/feature_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/gpu_tree_learner.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX shared library lib_lightgbm.so\u001b[0m\n",
            "[ 94%] Built target _lightgbm\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/serial_tree_learner.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/tree_learner.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lightgbm.dir/src/treelearner/voting_parallel_tree_learner.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lightgbm\u001b[0m\n",
            "[100%] Built target lightgbm\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython-all-dev python-all python-all-dev python-asn1crypto\n",
            "  python-cffi-backend python-crypto python-cryptography python-dbus\n",
            "  python-enum34 python-gi python-idna python-ipaddress python-keyring\n",
            "  python-keyrings.alt python-pip-whl python-pkg-resources python-secretstorage\n",
            "  python-setuptools python-six python-wheel python-xdg\n",
            "Suggested packages:\n",
            "  python-crypto-doc python-cryptography-doc python-cryptography-vectors\n",
            "  python-dbus-dbg python-dbus-doc python-enum34-doc python-gi-cairo\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0 python-fs\n",
            "  python-gdata python-keyczar python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  libpython-all-dev python-all python-all-dev python-asn1crypto\n",
            "  python-cffi-backend python-crypto python-cryptography python-dbus\n",
            "  python-enum34 python-gi python-idna python-ipaddress python-keyring\n",
            "  python-keyrings.alt python-pip python-pip-whl python-pkg-resources\n",
            "  python-secretstorage python-setuptools python-six python-wheel python-xdg\n",
            "0 upgraded, 22 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 3,431 kB of archives.\n",
            "After this operation, 10.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpython-all-dev amd64 2.7.15~rc1-1 [1,092 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-all amd64 2.7.15~rc1-1 [1,076 B]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-all-dev amd64 2.7.15~rc1-1 [1,100 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-asn1crypto all 0.24.0-1 [72.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-cffi-backend amd64 1.11.5-1 [63.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-enum34 all 1.1.6-2 [34.8 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-idna all 2.6-1 [32.4 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-ipaddress all 1.0.17-1 [18.2 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-six all 1.11.0-2 [11.3 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-cryptography amd64 2.1.4-1ubuntu1.4 [276 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-dbus amd64 1.2.6-1 [90.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python-gi amd64 3.26.1-2ubuntu1 [197 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-secretstorage all 2.3.1-2 [11.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-keyring all 10.6.0-1 [30.6 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-keyrings.alt all 3.0-1 [16.7 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.4 [1,653 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip all 9.0.1-2.3~ubuntu1.18.04.4 [151 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-setuptools all 39.0.1-2 [329 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-wheel all 0.30.0-0.2 [36.4 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-xdg all 0.25-4ubuntu1 [31.3 kB]\n",
            "Fetched 3,431 kB in 0s (22.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 22.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython-all-dev:amd64.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libpython-all-dev_2.7.15~rc1-1_amd64.deb ...\n",
            "Unpacking libpython-all-dev:amd64 (2.7.15~rc1-1) ...\n",
            "Selecting previously unselected package python-all.\n",
            "Preparing to unpack .../01-python-all_2.7.15~rc1-1_amd64.deb ...\n",
            "Unpacking python-all (2.7.15~rc1-1) ...\n",
            "Selecting previously unselected package python-all-dev.\n",
            "Preparing to unpack .../02-python-all-dev_2.7.15~rc1-1_amd64.deb ...\n",
            "Unpacking python-all-dev (2.7.15~rc1-1) ...\n",
            "Selecting previously unselected package python-asn1crypto.\n",
            "Preparing to unpack .../03-python-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python-cffi-backend.\n",
            "Preparing to unpack .../04-python-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python-crypto.\n",
            "Preparing to unpack .../05-python-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python-enum34.\n",
            "Preparing to unpack .../06-python-enum34_1.1.6-2_all.deb ...\n",
            "Unpacking python-enum34 (1.1.6-2) ...\n",
            "Selecting previously unselected package python-idna.\n",
            "Preparing to unpack .../07-python-idna_2.6-1_all.deb ...\n",
            "Unpacking python-idna (2.6-1) ...\n",
            "Selecting previously unselected package python-ipaddress.\n",
            "Preparing to unpack .../08-python-ipaddress_1.0.17-1_all.deb ...\n",
            "Unpacking python-ipaddress (1.0.17-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../09-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-cryptography.\n",
            "Preparing to unpack .../10-python-cryptography_2.1.4-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking python-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Selecting previously unselected package python-dbus.\n",
            "Preparing to unpack .../11-python-dbus_1.2.6-1_amd64.deb ...\n",
            "Unpacking python-dbus (1.2.6-1) ...\n",
            "Selecting previously unselected package python-gi.\n",
            "Preparing to unpack .../12-python-gi_3.26.1-2ubuntu1_amd64.deb ...\n",
            "Unpacking python-gi (3.26.1-2ubuntu1) ...\n",
            "Selecting previously unselected package python-secretstorage.\n",
            "Preparing to unpack .../13-python-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python-keyring.\n",
            "Preparing to unpack .../14-python-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python-keyrings.alt.\n",
            "Preparing to unpack .../15-python-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "Preparing to unpack .../16-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package python-pip.\n",
            "Preparing to unpack .../17-python-pip_9.0.1-2.3~ubuntu1.18.04.4_all.deb ...\n",
            "Unpacking python-pip (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../18-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-setuptools.\n",
            "Preparing to unpack .../19-python-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python-wheel.\n",
            "Preparing to unpack .../20-python-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python-xdg.\n",
            "Preparing to unpack .../21-python-xdg_0.25-4ubuntu1_all.deb ...\n",
            "Unpacking python-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python-idna (2.6-1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Setting up python-asn1crypto (0.24.0-1) ...\n",
            "Setting up python-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up python-wheel (0.30.0-0.2) ...\n",
            "Setting up libpython-all-dev:amd64 (2.7.15~rc1-1) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-cffi-backend (1.11.5-1) ...\n",
            "Setting up python-gi (3.26.1-2ubuntu1) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-enum34 (1.1.6-2) ...\n",
            "Setting up python-dbus (1.2.6-1) ...\n",
            "Setting up python-ipaddress (1.0.17-1) ...\n",
            "Setting up python-pip (9.0.1-2.3~ubuntu1.18.04.4) ...\n",
            "Setting up python-all (2.7.15~rc1-1) ...\n",
            "Setting up python-xdg (0.25-4ubuntu1) ...\n",
            "Setting up python-setuptools (39.0.1-2) ...\n",
            "Setting up python-keyrings.alt (3.0-1) ...\n",
            "Setting up python-all-dev (2.7.15~rc1-1) ...\n",
            "Setting up python-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Setting up python-secretstorage (2.3.1-2) ...\n",
            "Setting up python-keyring (10.6.0-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/f2/1489d3b6c72d68bf79cd0fba6b6c7497df4ebf7d40970e2d7eceb8d0ea9c/setuptools-51.0.0-py3-none-any.whl (785kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 14.0MB/s \n",
            "\u001b[?25hRequirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.1.4)\n",
            "Collecting numpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/86/753182c9085ba4936c0076269a571613387cdb77ae2bf537448bfd63472c/numpy-1.19.4-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 34.1MB/s \n",
            "\u001b[?25hCollecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9MB 1.2MB/s \n",
            "\u001b[?25hCollecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 63.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.17.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.5.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: setuptools, numpy, scipy, threadpoolctl, scikit-learn\n",
            "  Found existing installation: setuptools 50.3.2\n",
            "    Uninstalling setuptools-50.3.2:\n",
            "      Successfully uninstalled setuptools-50.3.2\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed numpy-1.19.4 scikit-learn-0.23.2 scipy-1.5.4 setuptools-51.0.0 threadpoolctl-2.1.0\n",
            "/content/LightGBM/python-package\n",
            "running install\n",
            "running build\n",
            "running build_py\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt\n",
            "INFO:root:Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/lightgbm\n",
            "copying lightgbm/basic.py -> build/lib/lightgbm\n",
            "copying lightgbm/compat.py -> build/lib/lightgbm\n",
            "copying lightgbm/callback.py -> build/lib/lightgbm\n",
            "copying lightgbm/libpath.py -> build/lib/lightgbm\n",
            "copying lightgbm/plotting.py -> build/lib/lightgbm\n",
            "copying lightgbm/engine.py -> build/lib/lightgbm\n",
            "copying lightgbm/__init__.py -> build/lib/lightgbm\n",
            "copying lightgbm/sklearn.py -> build/lib/lightgbm\n",
            "running egg_info\n",
            "creating lightgbm.egg-info\n",
            "writing lightgbm.egg-info/PKG-INFO\n",
            "writing dependency_links to lightgbm.egg-info/dependency_links.txt\n",
            "writing requirements to lightgbm.egg-info/requires.txt\n",
            "writing top-level names to lightgbm.egg-info/top_level.txt\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "no previously-included directories found matching 'build'\n",
            "warning: no files found matching 'LICENSE'\n",
            "warning: no files found matching '*.txt'\n",
            "warning: no files found matching '*.so' under directory 'lightgbm'\n",
            "warning: no files found matching '*.txt' under directory 'compile'\n",
            "warning: no files found matching '*.so' under directory 'compile'\n",
            "warning: no files found matching '*.dll' under directory 'compile/Release'\n",
            "warning: no files found matching '*.txt' under directory 'compile/compute'\n",
            "warning: no files found matching '*' under directory 'compile/compute/cmake'\n",
            "warning: no files found matching '*' under directory 'compile/compute/include'\n",
            "warning: no files found matching '*' under directory 'compile/compute/meta'\n",
            "warning: no files found matching '*' under directory 'compile/include'\n",
            "warning: no files found matching '*' under directory 'compile/src'\n",
            "warning: no files found matching 'LightGBM.sln' under directory 'compile/windows'\n",
            "warning: no files found matching 'LightGBM.vcxproj' under directory 'compile/windows'\n",
            "warning: no files found matching '*.dll' under directory 'compile/windows/x64/DLL'\n",
            "warning: no previously-included files matching '*.py[co]' found anywhere in distribution\n",
            "warning: no previously-included files found matching 'compile/compute/.git'\n",
            "writing manifest file 'lightgbm.egg-info/SOURCES.txt'\n",
            "copying lightgbm/VERSION.txt -> build/lib/lightgbm\n",
            "running install_lib\n",
            "copying build/lib/lightgbm/basic.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/compat.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/callback.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/libpath.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/plotting.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/VERSION.txt -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/engine.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/__init__.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "copying build/lib/lightgbm/sklearn.py -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "INFO:LightGBM:Installing lib_lightgbm from: ['../lib_lightgbm.so']\n",
            "copying ../lib_lightgbm.so -> /usr/local/lib/python3.6/dist-packages/lightgbm\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/basic.py to basic.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/compat.py to compat.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/callback.py to callback.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/libpath.py to libpath.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/plotting.py to plotting.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/engine.py to engine.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling /usr/local/lib/python3.6/dist-packages/lightgbm/sklearn.py to sklearn.cpython-36.pyc\n",
            "running install_egg_info\n",
            "Copying lightgbm.egg-info to /usr/local/lib/python3.6/dist-packages/lightgbm-3.1.0.99-py3.6.egg-info\n",
            "running install_scripts\n",
            "/content\n",
            "adc.json  sample_data\t\t save_items  train.csv\n",
            "LightGBM  sample_submission.csv  test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MHZOYou3F-0"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from contextlib import contextmanager\n",
        "import multiprocessing as mp\n",
        "from functools import partial\n",
        "from scipy.stats import kurtosis, iqr, skew\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "def main(debug= False):\n",
        "    num_rows = 30000 if debug else None\n",
        "    with timer(\"application_train and application_test\"):\n",
        "        df = get_train_test(DATA_DIRECTORY, num_rows= num_rows)\n",
        "        print(\"Application dataframe shape: \", df.shape)\n",
        "\n",
        "    df = reduce_memory(df)   \n",
        "    lgbm_categorical_feat = [\n",
        "    'CODE_GENDER', 'FLAG_OWN_CAR', 'NAME_CONTRACT_TYPE', 'NAME_EDUCATION_TYPE',\n",
        "    'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 'NAME_INCOME_TYPE', 'OCCUPATION_TYPE',\n",
        "    'ORGANIZATION_TYPE', 'NAME_TYPE_SUITE']\n",
        "    with timer(\"Run LightGBM\"):\n",
        "        feat_importance = kfold_lightgbm_sklearn(df, lgbm_categorical_feat)\n",
        "        print(feat_importance)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------- LIGHTGBM MODEL -------------------------\n",
        "\n",
        "def kfold_lightgbm_sklearn(data, categorical_feature = None):\n",
        "    df = data[data['TARGET'].notnull()]\n",
        "    test = data[data['TARGET'].isnull()]\n",
        "    print(\"Train/valid shape: {}, test shape: {}\".format(df.shape, test.shape))\n",
        "    del_features = ['TARGET', 'SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV', 'index', 'level_0']\n",
        "    predictors = list(filter(lambda v: v not in del_features, df.columns))\n",
        "\n",
        "    if not STRATIFIED_KFOLD:\n",
        "        folds = KFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
        "    else:\n",
        "        folds = StratifiedKFold(n_splits= NUM_FOLDS, shuffle=True, random_state= RANDOM_SEED)\n",
        "\n",
        "    # Hold oof predictions, test predictions, feature importance and training/valid auc\n",
        "    oof_preds = np.zeros(df.shape[0])\n",
        "    sub_preds = np.zeros(test.shape[0])\n",
        "    importance_df = pd.DataFrame()\n",
        "    eval_results = dict()\n",
        "\n",
        "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df[predictors], df['TARGET'])):\n",
        "        train_x, train_y = df[predictors].iloc[train_idx], df['TARGET'].iloc[train_idx]\n",
        "        valid_x, valid_y = df[predictors].iloc[valid_idx], df['TARGET'].iloc[valid_idx]\n",
        "\n",
        "\n",
        "        if use_save_model:\n",
        "            clf = pickle.load(open(f'{PRE_MODEL_DIR}/FOLD{n_fold}_.pickle', 'rb'))\n",
        "        else:\n",
        "            params = {'random_state': RANDOM_SEED, 'nthread': NUM_THREADS}\n",
        "            clf = LGBMClassifier(**{**params, **LIGHTGBM_PARAMS})\n",
        "            if not categorical_feature:\n",
        "                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
        "                        eval_metric='auc', verbose=400, early_stopping_rounds= EARLY_STOPPING)\n",
        "            else:\n",
        "                clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)],\n",
        "                        eval_metric='auc', verbose=400, early_stopping_rounds=EARLY_STOPPING,\n",
        "                        feature_name= list(df[predictors].columns), categorical_feature= categorical_feature)\n",
        "            pickle.dump(clf, open(f'save_items/FOLD{n_fold}_.pickle', 'wb'))\n",
        "            clf = pickle.load(open(f'save_items/FOLD{n_fold}_.pickle', 'rb'))\n",
        "\n",
        "\n",
        "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
        "        sub_preds += clf.predict_proba(test[predictors], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
        "\n",
        "        # Feature importance by GAIN and SPLIT\n",
        "        fold_importance = pd.DataFrame()\n",
        "        fold_importance[\"feature\"] = predictors\n",
        "        fold_importance[\"gain\"] = clf.booster_.feature_importance(importance_type='gain')\n",
        "        fold_importance[\"split\"] = clf.booster_.feature_importance(importance_type='split')\n",
        "        importance_df = pd.concat([importance_df, fold_importance], axis=0)\n",
        "        eval_results['train_{}'.format(n_fold+1)]  = clf.evals_result_['training']['auc']\n",
        "        eval_results['valid_{}'.format(n_fold+1)] = clf.evals_result_['valid_1']['auc']\n",
        "\n",
        "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
        "        del clf, train_x, train_y, valid_x, valid_y\n",
        "        gc.collect()\n",
        "\n",
        "    print('Full AUC score %.6f' % roc_auc_score(df['TARGET'], oof_preds))\n",
        "    test['TARGET'] = sub_preds.copy()\n",
        "\n",
        "    # Get the average feature importance between folds\n",
        "    mean_importance = importance_df.groupby('feature').mean().reset_index()\n",
        "    mean_importance.sort_values(by= 'gain', ascending=False, inplace=True)\n",
        "    # Save feature importance, test predictions and oof predictions as csv\n",
        "    if GENERATE_SUBMISSION_FILES:\n",
        "\n",
        "        # Generate oof csv\n",
        "        oof = pd.DataFrame()\n",
        "        oof['SK_ID_CURR'] = df['SK_ID_CURR'].copy()\n",
        "        df['PREDICTIONS'] = oof_preds.copy()\n",
        "        df['TARGET'] = df['TARGET'].copy()\n",
        "        df.to_csv('oof{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
        "        # Save submission (test data) and feature importance\n",
        "\n",
        "        # change\n",
        "        test[['SK_ID_CURR', 'TARGET']].to_csv('save_items/submission.csv', index=False)\n",
        "\n",
        "        # test[['SK_ID_CURR', 'TARGET']].to_csv('submission{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
        "        mean_importance.to_csv('feature_importance{}.csv'.format(SUBMISSION_SUFIX), index=False)\n",
        "    return mean_importance\n",
        "\n",
        "\n",
        "# ------------------------- APPLICATION PIPELINE -------------------------\n",
        "\n",
        "def get_train_test(path, num_rows = None):\n",
        "    \"\"\" Process application_train.csv and application_test.csv and return a pandas dataframe. \"\"\"\n",
        "    train = pd.read_csv(os.path.join(path, 'train.csv'), nrows= num_rows)\n",
        "    test = pd.read_csv(os.path.join(path, 'test.csv'), nrows= num_rows)\n",
        "    df = train.append(test)\n",
        "    del train, test; gc.collect()\n",
        "    # Data cleaning\n",
        "    df = df[df['CODE_GENDER'] != 'XNA']  # 4 people with XNA code gender\n",
        "\n",
        "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace=True)\n",
        "    df['DAYS_LAST_PHONE_CHANGE'].replace(0, np.nan, inplace=True)\n",
        "\n",
        "    # Categorical age - based on target=1 plot\n",
        "    df['AGE_RANGE'] = df['DAYS_BIRTH'].apply(lambda x: get_age_label(x))\n",
        "\n",
        "    # New features based on External sources\n",
        "    df['EXT_SOURCES_PROD'] = df['EXT_SOURCE_1'] * df['EXT_SOURCE_2'] * df['EXT_SOURCE_3']\n",
        "    df['EXT_SOURCES_WEIGHTED'] = df.EXT_SOURCE_1 * 2 + df.EXT_SOURCE_2 * 1 + df.EXT_SOURCE_3 * 3\n",
        "    np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n",
        "    for function_name in ['min', 'max', 'mean', 'nanmedian', 'var']:\n",
        "        feature_name = 'EXT_SOURCES_{}'.format(function_name.upper())\n",
        "        df[feature_name] = eval('np.{}'.format(function_name))(\n",
        "            df[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']], axis=1)\n",
        "\n",
        "    # Credit ratios\n",
        "    df['CREDIT_TO_ANNUITY_RATIO'] = df['AMT_CREDIT'] / df['AMT_ANNUITY']\n",
        "    df['CREDIT_TO_GOODS_RATIO'] = df['AMT_CREDIT'] / df['AMT_GOODS_PRICE']\n",
        "    # Income ratios\n",
        "    df['ANNUITY_TO_INCOME_RATIO'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
        "    df['CREDIT_TO_INCOME_RATIO'] = df['AMT_CREDIT'] / df['AMT_INCOME_TOTAL']\n",
        "    df['INCOME_TO_EMPLOYED_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_EMPLOYED']\n",
        "    df['INCOME_TO_BIRTH_RATIO'] = df['AMT_INCOME_TOTAL'] / df['DAYS_BIRTH']\n",
        "    # Time ratios\n",
        "    df['EMPLOYED_TO_BIRTH_RATIO'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
        "    df['ID_TO_BIRTH_RATIO'] = df['DAYS_ID_PUBLISH'] / df['DAYS_BIRTH']\n",
        "    df['CAR_TO_BIRTH_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_BIRTH']\n",
        "    df['CAR_TO_EMPLOYED_RATIO'] = df['OWN_CAR_AGE'] / df['DAYS_EMPLOYED']\n",
        "    df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / df['DAYS_BIRTH']\n",
        "\n",
        "    # Groupby: Statistics for applications in the same group\n",
        "    group = ['ORGANIZATION_TYPE', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'AGE_RANGE', 'CODE_GENDER']\n",
        "    df = do_median(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_MEDIAN')\n",
        "    df = do_std(df, group, 'EXT_SOURCES_MEAN', 'GROUP_EXT_SOURCES_STD')\n",
        "    df = do_mean(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_MEAN')\n",
        "    df = do_std(df, group, 'AMT_INCOME_TOTAL', 'GROUP_INCOME_STD')\n",
        "    df = do_mean(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_MEAN')\n",
        "    df = do_std(df, group, 'CREDIT_TO_ANNUITY_RATIO', 'GROUP_CREDIT_TO_ANNUITY_STD')\n",
        "    df = do_mean(df, group, 'AMT_CREDIT', 'GROUP_CREDIT_MEAN')\n",
        "    df = do_mean(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_MEAN')\n",
        "    df = do_std(df, group, 'AMT_ANNUITY', 'GROUP_ANNUITY_STD')\n",
        "\n",
        "    # Encode categorical features (LabelEncoder)\n",
        "    df, le_encoded_cols = label_encoder(df, None)\n",
        "    df = drop_application_columns(df)\n",
        "    return df\n",
        "\n",
        "\n",
        "def drop_application_columns(df):\n",
        "    \"\"\" Drop features based on permutation feature importance. \"\"\"\n",
        "\n",
        "\n",
        "    drop_list = [\n",
        "        'CNT_CHILDREN', 'CNT_FAM_MEMBERS',\n",
        "        'FLAG_EMP_PHONE', 'FLAG_MOBIL', 'FLAG_CONT_MOBILE', 'FLAG_EMAIL', 'FLAG_PHONE',\n",
        "        'FLAG_OWN_REALTY', 'REG_REGION_NOT_LIVE_REGION', 'REG_REGION_NOT_WORK_REGION',\n",
        "        'REG_CITY_NOT_WORK_CITY', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OBS_60_CNT_SOCIAL_CIRCLE',\n",
        "        'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_YEAR',\n",
        "    ]\n",
        "\n",
        "    df.drop(drop_list, axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def get_age_label(days_birth):\n",
        "    \"\"\" Return the age group label (int). \"\"\"\n",
        "    age_years = -days_birth / 365\n",
        "    if age_years < 27: return 1\n",
        "    elif age_years < 40: return 2\n",
        "    elif age_years < 50: return 3\n",
        "    elif age_years < 65: return 4\n",
        "    elif age_years < 99: return 5\n",
        "    else: return 0\n",
        "\n",
        "# ------------------------- UTILITY FUNCTIONS -------------------------\n",
        "\n",
        "@contextmanager\n",
        "def timer(name):\n",
        "    t0 = time.time()\n",
        "    yield\n",
        "    print(\"{} - done in {:.0f}s\".format(name, time.time() - t0))\n",
        "\n",
        "\n",
        "def group(df_to_agg, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = df_to_agg.groupby(aggregate_by).agg(aggregations)\n",
        "    agg_df.columns = pd.Index(['{}{}_{}'.format(prefix, e[0], e[1].upper())\n",
        "                               for e in agg_df.columns.tolist()])\n",
        "    return agg_df.reset_index()\n",
        "\n",
        "\n",
        "def group_and_merge(df_to_agg, df_to_merge, prefix, aggregations, aggregate_by= 'SK_ID_CURR'):\n",
        "    agg_df = group(df_to_agg, prefix, aggregations, aggregate_by= aggregate_by)\n",
        "    return df_to_merge.merge(agg_df, how='left', on= aggregate_by)\n",
        "\n",
        "\n",
        "def do_mean(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].mean().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def do_median(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].median().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def do_std(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].std().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def do_sum(df, group_cols, counted, agg_name):\n",
        "    gp = df[group_cols + [counted]].groupby(group_cols)[counted].sum().reset_index().rename(\n",
        "        columns={counted: agg_name})\n",
        "    df = df.merge(gp, on=group_cols, how='left')\n",
        "    del gp\n",
        "    gc.collect()\n",
        "    return df\n",
        "\n",
        "\n",
        "def one_hot_encoder(df, categorical_columns=None, nan_as_category=True):\n",
        "    \"\"\"Create a new column for each categorical value in categorical columns. \"\"\"\n",
        "    original_columns = list(df.columns)\n",
        "    if not categorical_columns:\n",
        "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, dummy_na=nan_as_category)\n",
        "    categorical_columns = [c for c in df.columns if c not in original_columns]\n",
        "    return df, categorical_columns\n",
        "\n",
        "\n",
        "def label_encoder(df, categorical_columns=None):\n",
        "    \"\"\"Encode categorical values as integers (0,1,2,3...) with pandas.factorize. \"\"\"\n",
        "    if not categorical_columns:\n",
        "        categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "    for col in categorical_columns:\n",
        "        df[col], uniques = pd.factorize(df[col])\n",
        "    return df, categorical_columns\n",
        "\n",
        "\n",
        "\n",
        "def reduce_memory(df):\n",
        "    \"\"\"Reduce memory usage of a dataframe by setting data types. \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    print('Initial df memory usage is {:.2f} MB for {} columns'\n",
        "          .format(start_mem, len(df.columns)))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type != object:\n",
        "            cmin = df[col].min()\n",
        "            cmax = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                # Can use unsigned int here too\n",
        "                if cmin > np.iinfo(np.int8).min and cmax < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif cmin > np.iinfo(np.int16).min and cmax < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif cmin > np.iinfo(np.int32).min and cmax < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif cmin > np.iinfo(np.int64).min and cmax < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                if cmin > np.finfo(np.float16).min and cmax < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif cmin > np.finfo(np.float32).min and cmax < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "    memory_reduction = 100 * (start_mem - end_mem) / start_mem\n",
        "    print('Final memory usage is: {:.2f} MB - decreased by {:.1f}%'.format(end_mem, memory_reduction))\n",
        "    return df\n",
        "\n",
        "# ------------------------- CONFIGURATIONS -------------------------\n",
        "\n",
        "# GENERAL CONFIGURATIONS\n",
        "NUM_THREADS = 4\n",
        "DATA_DIRECTORY = \"./\"\n",
        "PRE_MODEL_DIR = '/content/use_model_dir/content/save_items'\n",
        "SUBMISSION_SUFIX = \"for_GCI\"\n",
        "\n",
        "# INSTALLMENTS TREND PERIODS\n",
        "INSTALLMENTS_LAST_K_TREND_PERIODS =  [12, 24, 60, 120]\n",
        "\n",
        "# LIGHTGBM CONFIGURATION AND HYPER-PARAMETERS\n",
        "GENERATE_SUBMISSION_FILES = True\n",
        "STRATIFIED_KFOLD = False\n",
        "\n",
        "RANDOM_SEED = 737851\n",
        "NUM_FOLDS = 10\n",
        "EARLY_STOPPING = 2500\n",
        "\n",
        "\n",
        "\n",
        "LIGHTGBM_PARAMS = { 'n_estimators': 100000, 'device' : 'gpu', 'subsample':1,'verbosity': -1, 'learning_rate': 0.001134, 'boosting': 'goss', 'num_leaves': 51, 'reg_alpha': 0.9957684513557233, 'reg_lambda': 0.9960067587898466, 'colsample_bytree': 0.17072069024645947, 'min_split_gain': 1.6977135811936967e-05, 'top_rate': 0.42562202305456304, 'other_rate': 0.543885726442511} \n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pd.set_option('display.max_rows', 60)\n",
        "    pd.set_option('display.max_columns', 100)\n",
        "    with timer(\"Pipeline total time\"):\n",
        "        main(debug= False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQgQHUCzCxHy"
      },
      "source": [
        "!zip -r /content/save_items.zip /content/save_items\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/save_items.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}